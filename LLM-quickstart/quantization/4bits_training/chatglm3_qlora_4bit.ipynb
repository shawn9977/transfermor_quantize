{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGLM3 Lora 实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intel/miniforge3/envs/peft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(\"../data/alpaca_data_zh/\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。',\n",
       "  '4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。',\n",
       "  '朱利叶斯·凯撒，又称尤利乌斯·恺撒（Julius Caesar）是古罗马的政治家、军事家和作家。他于公元前44年3月15日被刺杀。 \\n\\n根据历史记载，当时罗马元老院里一些参议员联合起来策划了对恺撒的刺杀行动，因为他们担心恺撒的统治将给罗马共和制带来威胁。在公元前44年3月15日（又称“3月的艾达之日”），恺撒去参加元老院会议时，被一群参议员包围并被攻击致死。据记载，他身中23刀，其中一刀最终致命。'],\n",
       " 'input': ['', '输入：4/16', ''],\n",
       " 'instruction': ['保持健康的三个提示。', '解释为什么以下分数等同于1/4', '朱利叶斯·凯撒是如何死亡的？']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMTokenizer(name_or_path='/home/intel/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/91a0561caa089280e94bf26a9fc3530482f0fe60', vocab_size=64798, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t64790: AddedToken(\"[gMASK]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t64792: AddedToken(\"sop\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t64795: AddedToken(\"<|user|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t64796: AddedToken(\"<|assistant|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/intel/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/91a0561caa089280e94bf26a9fc3530482f0fe60\", trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': [64790, 64792, 2893, 30917, 30994], 'attention_mask': [1, 1, 1, 1, 1], 'position_ids': [0, 1, 2, 3, 4]},\n",
       " 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(tokenizer.eos_token), tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 256\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = \"\\n\".join([example[\"instruction\"], example[\"input\"]]).strip()     # query\n",
    "    instruction = tokenizer.build_chat_input(instruction, history=[], role=\"user\")  # [gMASK]sop<|user|> \\n query<|assistant|>\n",
    "    response = tokenizer(\"\\n\" + example[\"output\"], add_special_tokens=False)        # \\n response, 缺少eos token\n",
    "    input_ids = instruction[\"input_ids\"][0].numpy().tolist() + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"][0].numpy().tolist() + response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"][0].numpy().tolist()) + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[gMASK] sop <|user|> \\n 解释为什么以下分数等同于1/4\\n输入：4/16 <|assistant|> \\n4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_ds[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:05<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 多卡情况，可以去掉device_map=\"auto\"，否则会将模型拆开，导致训练出问题\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/intel/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/91a0561caa089280e94bf26a9fc3530482f0fe60\", trust_remote_code=True, low_cpu_mem_usage=True, \n",
    "                                             torch_dtype=torch.bfloat16, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                             bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.embedding.word_embeddings.weight torch.bfloat16\n",
      "transformer.encoder.layers.0.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.0.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.0.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.0.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.0.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.0.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.0.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.1.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.1.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.1.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.1.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.1.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.1.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.1.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.2.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.2.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.2.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.2.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.2.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.2.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.2.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.3.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.3.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.3.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.3.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.3.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.3.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.3.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.4.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.4.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.4.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.4.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.4.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.4.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.4.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.5.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.5.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.5.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.5.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.5.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.5.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.5.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.6.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.6.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.6.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.6.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.6.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.6.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.6.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.7.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.7.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.7.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.7.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.7.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.7.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.7.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.8.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.8.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.8.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.8.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.8.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.8.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.8.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.9.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.9.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.9.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.9.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.9.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.9.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.9.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.10.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.10.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.10.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.10.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.10.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.10.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.10.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.11.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.11.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.11.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.11.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.11.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.11.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.11.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.12.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.12.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.12.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.12.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.12.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.12.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.12.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.13.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.13.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.13.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.13.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.13.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.13.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.13.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.14.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.14.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.14.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.14.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.14.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.14.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.14.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.15.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.15.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.15.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.15.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.15.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.15.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.15.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.16.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.16.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.16.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.16.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.16.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.16.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.16.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.17.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.17.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.17.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.17.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.17.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.17.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.17.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.18.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.18.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.18.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.18.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.18.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.18.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.18.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.19.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.19.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.19.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.19.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.19.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.19.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.19.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.20.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.20.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.20.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.20.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.20.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.20.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.20.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.21.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.21.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.21.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.21.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.21.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.21.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.21.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.22.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.22.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.22.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.22.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.22.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.22.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.22.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.23.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.23.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.23.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.23.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.23.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.23.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.23.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.24.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.24.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.24.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.24.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.24.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.24.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.24.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.25.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.25.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.25.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.25.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.25.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.25.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.25.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.26.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.26.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.26.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.26.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.26.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.26.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.26.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.layers.27.input_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.27.self_attention.query_key_value.weight torch.uint8\n",
      "transformer.encoder.layers.27.self_attention.query_key_value.bias torch.bfloat16\n",
      "transformer.encoder.layers.27.self_attention.dense.weight torch.uint8\n",
      "transformer.encoder.layers.27.post_attention_layernorm.weight torch.bfloat16\n",
      "transformer.encoder.layers.27.mlp.dense_h_to_4h.weight torch.uint8\n",
      "transformer.encoder.layers.27.mlp.dense_4h_to_h.weight torch.uint8\n",
      "transformer.encoder.final_layernorm.weight torch.bfloat16\n",
      "transformer.output_layer.weight torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Step1 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "\n",
    "config = LoraConfig(target_modules=[\"query_key_value\"])\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Step2 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./chatbot\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_ds.select(range(6000)),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./chatbot/tokenizer_config.json',\n",
       " './chatbot/special_tokens_map.json',\n",
       " './chatbot/tokenizer.model',\n",
       " './chatbot/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainer.save_model()\n",
    "model.save_pretrained(\"./chatbot\")\n",
    "tokenizer.save_pretrained(\"./chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "想要在数学考试中获得高分，需要掌握以下几点：\n",
      "\n",
      "1. 扎实的基本功：数学考试要求熟练掌握基础知识，如加减乘除、分数、百分数、三角函数、代数等。基础扎实才能在考试中迅速解题。\n",
      "\n",
      "2. 解题技巧：掌握解题技巧和方法，如先易后难、逻辑推理、归纳法、记忆方法等。通过多练习，熟练运用这些技巧，提高解题速度和正确率。\n",
      "\n",
      "3. 题目分析：在考试中，题目分析很重要。仔细阅读题目，理解题目要求，找出解题的关键点，制定解题策略。\n",
      "\n",
      "4. 时间分配：合理分配考试时间，先做会做的题目，不会做的题目暂且放慢，合理分配时间，避免因时间紧张导致失分。\n",
      "\n",
      "5. 复习和练习：定期进行复习和练习，查漏补缺。多做习题，总结经验，巩固基础知识。\n",
      "\n",
      "6. 考试心态：保持积极的心态，考试过程中保持冷静，遇到难题不慌张。相信自己，尽力而为。\n",
      "\n",
      "7. 及时总结：每次考试后，总结自己的错误和不足，找出原因，针对性地进行改进。\n",
      "\n",
      "8. 合理安排学习时间：数学需要较强的逻辑性，需要花费一定的时间来理解和掌握。要合理安排学习时间，保证数学学习的时间和质量。\n",
      "\n",
      "只要努力学习，掌握方法，相信你会在数学考试中取得好成绩。\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(model.chat(tokenizer, \"数学考试怎么考高分？\", history=[])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intel/miniforge3/envs/peft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"/home/intel/project/quantize/transformer/chatbot\"\n",
    "\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "print(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京，简称“京”，是中华人民共和国的首都、直辖市、国家中心城市、超大城市，国务院首批沿海开放城市。位于华北地区，背靠燕山，毗邻天津市和河北省。全市总面积1.6万平方公里，常住人口约为2170万（截至2020年末）。\n",
      "\n",
      "北京是我国的政治、文化、科技创新、国际交往和综合交通枢纽中心，拥有世界著名的文化古迹和自然景观。自古以来就是著名的历史文化名城，有着3000多年的历史，是元、明、清三个朝代的都城。北京有着丰富的旅游资源，包括故宫、长城、颐和园、天坛、北海公园等世界文化遗产，以及天安门、颐和园、圆明园等著名景点。\n",
      "\n",
      "北京也是我国的教育和科研重镇，拥有清华大学、北京大学等著名高校，以及中国科学院、中国科学技术院等国家级科研机构。此外，北京还是2022年冬奥会的举办地，届时将成为全球关注的焦点。\n",
      "\n",
      "作为中国的一个重要城市，北京经济实力雄厚，是我国GDP排名前列的城市之一。近年来，北京致力于发展现代服务业、高科技产业和创新产业，打造国际化、法治化、现代化的都市。同时，北京也在努力改善生态环境，推动城市可持续发展。\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(model.chat(tokenizer, \"介绍北京？\", history=[])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers              4.37.2\n",
      "tokenizers                0.15.2\n",
      "Package                   Version\n",
      "------------------------- ------------\n",
      "accelerate                0.26.1\n",
      "aiofiles                  23.2.1\n",
      "aiohappyeyeballs          2.4.3\n",
      "aiohttp                   3.10.10\n",
      "aiosignal                 1.3.1\n",
      "altair                    5.4.1\n",
      "annotated-types           0.7.0\n",
      "anyio                     4.6.2.post1\n",
      "asttokens                 2.4.1\n",
      "async-timeout             4.0.3\n",
      "attrs                     24.2.0\n",
      "audioread                 3.0.1\n",
      "auto-gptq                 0.6.0\n",
      "autoawq                   0.2.2\n",
      "autoawq_kernels           0.0.7\n",
      "bitsandbytes              0.41.3.post2\n",
      "certifi                   2024.8.30\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.0\n",
      "click                     8.1.7\n",
      "coloredlogs               15.0.1\n",
      "comm                      0.2.2\n",
      "contourpy                 1.3.0\n",
      "cycler                    0.12.1\n",
      "dataclasses-json          0.6.7\n",
      "datasets                  2.16.1\n",
      "debugpy                   1.8.7\n",
      "decorator                 5.1.1\n",
      "dill                      0.3.7\n",
      "distro                    1.9.0\n",
      "docstring_parser          0.16\n",
      "evaluate                  0.4.1\n",
      "exceptiongroup            1.2.2\n",
      "executing                 2.1.0\n",
      "fastapi                   0.115.4\n",
      "ffmpeg                    1.4\n",
      "ffmpeg-python             0.2.0\n",
      "ffmpy                     0.4.0\n",
      "filelock                  3.16.1\n",
      "fonttools                 4.54.1\n",
      "frozenlist                1.5.0\n",
      "fsspec                    2023.10.0\n",
      "future                    1.0.0\n",
      "gekko                     1.2.1\n",
      "gradio                    4.13.0\n",
      "gradio_client             0.8.0\n",
      "greenlet                  3.1.1\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.6\n",
      "httpx                     0.27.2\n",
      "huggingface-hub           0.26.2\n",
      "humanfriendly             10.0\n",
      "idna                      3.10\n",
      "importlib_metadata        8.5.0\n",
      "importlib_resources       6.4.5\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.29.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.4\n",
      "jiwer                     3.0.3\n",
      "joblib                    1.4.2\n",
      "jsonpatch                 1.33\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter_core              5.7.2\n",
      "kiwisolver                1.4.7\n",
      "langchain                 0.2.0\n",
      "langchain-community       0.2.0\n",
      "langchain-core            0.2.1\n",
      "langchain-openai          0.1.7\n",
      "langchain-text-splitters  0.2.1\n",
      "langsmith                 0.1.140\n",
      "lazy_loader               0.4\n",
      "librosa                   0.10.1\n",
      "llvmlite                  0.43.0\n",
      "markdown-it-py            3.0.0\n",
      "MarkupSafe                2.1.5\n",
      "marshmallow               3.23.1\n",
      "matplotlib                3.9.2\n",
      "matplotlib-inline         0.1.7\n",
      "mdurl                     0.1.2\n",
      "mpmath                    1.3.0\n",
      "msgpack                   1.1.0\n",
      "multidict                 6.1.0\n",
      "multiprocess              0.70.15\n",
      "mypy-extensions           1.0.0\n",
      "narwhals                  1.13.2\n",
      "nest_asyncio              1.6.0\n",
      "networkx                  3.4.2\n",
      "numba                     0.60.0\n",
      "numpy                     1.26.4\n",
      "nvidia-cublas-cu12        12.1.3.1\n",
      "nvidia-cuda-cupti-cu12    12.1.105\n",
      "nvidia-cuda-nvrtc-cu12    12.1.105\n",
      "nvidia-cuda-runtime-cu12  12.1.105\n",
      "nvidia-cudnn-cu12         8.9.2.26\n",
      "nvidia-cufft-cu12         11.0.2.54\n",
      "nvidia-curand-cu12        10.3.2.106\n",
      "nvidia-cusolver-cu12      11.4.5.107\n",
      "nvidia-cusparse-cu12      12.1.0.106\n",
      "nvidia-nccl-cu12          2.20.5\n",
      "nvidia-nvjitlink-cu12     12.6.77\n",
      "nvidia-nvtx-cu12          12.1.105\n",
      "openai                    1.30.1\n",
      "optimum                   1.17.0\n",
      "orjson                    3.10.11\n",
      "packaging                 23.2\n",
      "pandas                    2.1.1\n",
      "parso                     0.8.4\n",
      "peft                      0.7.1\n",
      "pexpect                   4.9.0\n",
      "pickleshare               0.7.5\n",
      "pillow                    10.4.0\n",
      "pip                       24.3.1\n",
      "platformdirs              4.3.6\n",
      "pooch                     1.8.2\n",
      "prompt_toolkit            3.0.48\n",
      "propcache                 0.2.0\n",
      "protobuf                  5.28.3\n",
      "psutil                    6.1.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pyarrow                   18.0.0\n",
      "pyarrow-hotfix            0.6\n",
      "pycparser                 2.22\n",
      "pydantic                  2.9.2\n",
      "pydantic_core             2.23.4\n",
      "pydub                     0.25.1\n",
      "Pygments                  2.18.0\n",
      "pyparsing                 3.2.0\n",
      "python-dateutil           2.9.0\n",
      "python-multipart          0.0.17\n",
      "pytz                      2024.2\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.0\n",
      "RapidFuzz                 3.10.1\n",
      "referencing               0.35.1\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.3\n",
      "requests-toolbelt         1.0.0\n",
      "responses                 0.18.0\n",
      "rich                      13.9.4\n",
      "rouge                     1.0.1\n",
      "rpds-py                   0.21.0\n",
      "safetensors               0.4.5\n",
      "scikit-learn              1.3.2\n",
      "scipy                     1.14.1\n",
      "semantic-version          2.10.0\n",
      "sentencepiece             0.2.0\n",
      "setuptools                75.3.0\n",
      "shellingham               1.5.4\n",
      "shtab                     1.7.1\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soundfile                 0.12.1\n",
      "soxr                      0.5.0.post1\n",
      "SQLAlchemy                2.0.36\n",
      "stack-data                0.6.2\n",
      "starlette                 0.41.2\n",
      "sympy                     1.13.3\n",
      "tenacity                  8.5.0\n",
      "threadpoolctl             3.5.0\n",
      "tiktoken                  0.8.0\n",
      "timm                      0.9.12\n",
      "tokenizers                0.15.2\n",
      "tomlkit                   0.12.0\n",
      "torch                     2.3.1\n",
      "torchvision               0.18.1\n",
      "tornado                   6.4.1\n",
      "tqdm                      4.67.0\n",
      "traitlets                 5.14.3\n",
      "transformers              4.37.2\n",
      "triton                    2.3.1\n",
      "trl                       0.8.1\n",
      "typer                     0.12.5\n",
      "typing_extensions         4.12.2\n",
      "typing-inspect            0.9.0\n",
      "tyro                      0.8.14\n",
      "tzdata                    2024.2\n",
      "urllib3                   2.2.3\n",
      "uvicorn                   0.32.0\n",
      "wcwidth                   0.2.13\n",
      "websockets                11.0.3\n",
      "wheel                     0.44.0\n",
      "xxhash                    3.5.0\n",
      "yarl                      1.17.1\n",
      "zipp                      3.20.2\n",
      "zstandard                 0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip list|grep transformers\n",
    "!pip list|grep tokenizer\n",
    "!pip list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
