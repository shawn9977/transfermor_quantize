{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c9be60-bb96-4af9-a5ee-1eb47b201d45",
   "metadata": {},
   "source": [
    "# Transformers 量化技术 BitsAndBytes\n",
    "\n",
    "![](docs/images/qlora.png)\n",
    "\n",
    "`bitsandbytes`是将模型量化为8位和4位的最简单选择。 \n",
    "\n",
    "- 8位量化将fp16中的异常值与int8中的非异常值相乘，将非异常值转换回fp16，然后将它们相加以返回fp16中的权重。这减少了异常值对模型性能产生的降级效果。\n",
    "- 4位量化进一步压缩了模型，并且通常与QLoRA一起用于微调量化LLM（低精度语言模型）。\n",
    "\n",
    "（`异常值`是指大于某个阈值的隐藏状态值，这些值是以fp16进行计算的。虽然这些值通常服从正态分布（[-3.5, 3.5]），但对于大型模型来说，该分布可能会有很大差异（[-60, 6]或[6, 60]）。8位量化适用于约为5左右的数值，但超过此范围后将导致显著性能损失。一个好的默认阈值是6，但对于不稳定的模型（小型模型或微调）可能需要更低的阈值。）\n",
    "\n",
    "## 在 Transformers 中使用参数量化\n",
    "\n",
    "使用 Transformers 库的 `model.from_pretrained()`方法中的`load_in_8bit`或`load_in_4bit`参数，便可以对模型进行量化。只要模型支持使用Accelerate加载并包含torch.nn.Linear层，这几乎适用于任何模态的任何模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2385671-3e67-4fcb-9243-d4b1affea031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intel/miniforge3/envs/peft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:04<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_id = \"/home/intel/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/91a0561caa089280e94bf26a9fc3530482f0fe60\"\n",
    "#model_id = \"/home/intel/models/qwen2-7B-instruct\"\n",
    "#model_id = \"facebook/opt-2.7b\"\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                  device_map=\"auto\",\n",
    "                                                  load_in_4bit=True,\n",
    "                                                  torch_dtype=torch.float16,\n",
    "                                                  trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4731ac5-fe26-471e-ad17-eb2ba42cb596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198b216-b113-4851-a02f-f57be038e1ac",
   "metadata": {},
   "source": [
    "### 实测GPU显存占用：Int4 量化精度\n",
    "\n",
    "```shell\n",
    "Sun Dec 24 18:04:14 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   42C    P0              26W /  70W |   1779MiB / 15360MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d504b78-9ea4-4100-b614-03dc3bbcb65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前模型占用的 GPU显存（差值为预留给 PyTorch 的显存）\n",
    "memory_footprint_bytes = model_4bit.get_memory_footprint()\n",
    "memory_footprint_mib = memory_footprint_bytes / (1024 ** 2)  # 转换为 MiB\n",
    "\n",
    "print(f\"{memory_footprint_mib:.2f}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d39c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "quant_model_dir =\"models/chaglm3-6b-bnd\"\n",
    "\n",
    "# 保存模型权重\n",
    "model_4bit.save_pretrained(quant_model_dir)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(quant_model_dir)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2edef-9142-443b-b55c-b57872a1fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_model_dir, trust_remote_code=True)\n",
    "\n",
    "text = \"介绍下上海\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model_4bit.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0117f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_4bit.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400066bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_model_dir,trust_remote_code=True)\n",
    "model_4 = AutoModelForCausalLM.from_pretrained(quant_model_dir, trust_remote_code=True, device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fbe9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model_4.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc455f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(\"介绍下北京\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f299ea-77f6-45cc-82c9-87c96addda06",
   "metadata": {},
   "source": [
    "### 使用 NF4 精度加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00249404-c60b-47a5-bcb9-a8a4b4b6266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab499752-4c53-4ab4-a6a1-1fdf88cbbd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前模型占用的 GPU显存（差值为预留给 PyTorch 的显存）\n",
    "memory_footprint_bytes = model_nf4.get_memory_footprint()\n",
    "memory_footprint_mib = memory_footprint_bytes / (1024 ** 2)  # 转换为 MiB\n",
    "\n",
    "print(f\"{memory_footprint_mib:.2f}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b01b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "quant_model_nf4_dir =\"models/chaglm3-6b-bnd_nf4\"\n",
    "\n",
    "# 保存模型权重\n",
    "model_nf4.save_pretrained(quant_model_nf4_dir)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(quant_model_nf4_dir)  \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_model_nf4_dir, trust_remote_code=True)\n",
    "\n",
    "text = \"介绍下上海\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model_nf4.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "model_nf4.eval()\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_n = AutoModelForCausalLM.from_pretrained(quant_model_nf4_dir, trust_remote_code=True, device_map=\"cuda\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2077ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model_n.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(\"介绍下湖南\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d335c9-9f13-4834-8008-af20a9f5ca56",
   "metadata": {},
   "source": [
    "### 使用双量化加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfa211-9ad8-4c7b-93a8-37cccaad975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "double_quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant_config, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3913a-a4aa-4d65-8901-8bcf546f1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前模型占用的 GPU显存（差值为预留给 PyTorch 的显存）\n",
    "memory_footprint_bytes = model_double_quant.get_memory_footprint()\n",
    "memory_footprint_mib = memory_footprint_bytes / (1024 ** 2)  # 转换为 MiB\n",
    "\n",
    "print(f\"{memory_footprint_mib:.2f}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "quant_model_2q_dir =\"models/chaglm3-6b-bnd_2q\"\n",
    "\n",
    "# 保存模型权重\n",
    "model_double_quant.save_pretrained(quant_model_2q_dir)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(quant_model_2q_dir)  \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_model_2q_dir, trust_remote_code=True)\n",
    "\n",
    "text = \"介绍下河北\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model_double_quant.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "model_double_quant.eval()\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(quant_model_2q_dir, trust_remote_code=True, device_map=\"cuda\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0788cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model_2.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(\"介绍下河南\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8153e9d-a080-47df-af83-3f1582b2b367",
   "metadata": {},
   "source": [
    "### 使用 QLoRA 所有量化技术加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4bd4f3a-a7f9-4545-b6a9-732fd6f91b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "qlora_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_qlora = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=qlora_config, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da8edf77-03cc-4303-a3c0-1b088e5ec958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3739.69MiB\n"
     ]
    }
   ],
   "source": [
    "# 获取当前模型占用的 GPU显存（差值为预留给 PyTorch 的显存）\n",
    "memory_footprint_bytes = model_qlora.get_memory_footprint()\n",
    "memory_footprint_mib = memory_footprint_bytes / (1024 ** 2)  # 转换为 MiB\n",
    "\n",
    "print(f\"{memory_footprint_mib:.2f}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf04637-85f6-4ef1-a1bd-81448cd9325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK] sop 介绍下山东论语\n",
      "《山东论语》是明朝作家李时中创作的一部儒家经典著作。它是对孔子的思想、言论和行为的总结和概括，旨在传承和弘扬儒家文化。\n",
      "\n",
      "《山东论语》共分为二十篇，每一篇都涵盖了不同的主题，如孝道、忠诚、仁爱\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "quant_model_qlora_dir =\"models/chaglm3-6b-bnd_qlora\"\n",
    "\n",
    "# 保存模型权重\n",
    "model_qlora.save_pretrained(quant_model_qlora_dir)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(quant_model_qlora_dir)  \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_model_qlora_dir, trust_remote_code=True)\n",
    "\n",
    "text = \"介绍下山东\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model_qlora.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "model_qlora.eval()\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_q = AutoModelForCausalLM.from_pretrained(quant_model_qlora_dir, trust_remote_code=True, device_map=\"cuda\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1cdb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model_q.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6b9751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK] sop 介绍下陕西西安的美食文化\n",
      "西安是中国西部地区的重要城市,也是中国历史文化名城之一。西安的美食文化悠久而丰富,是中国美食文化的重要组成部分。\n",
      "\n",
      "西安的美食以面食为主,其中最有名的是羊肉泡馍、肉夹馍、凉皮、油泼面等。羊肉泡馍\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"介绍下陕西\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
